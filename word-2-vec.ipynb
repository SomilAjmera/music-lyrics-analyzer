{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf4c6e37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "cf4c6e37",
    "outputId": "1a82f6f7-864b-489c-a417-2f4015698d56"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      4\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AMv2ihSjjFrl",
   "metadata": {
    "id": "AMv2ihSjjFrl"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f22010",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02f22010",
    "outputId": "b3cd8f61-6c91-4ccf-f179-26e80f0e2fa0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\somil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\somil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\somil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download NLTK data (stopwords and WordNet)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5491a4ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5491a4ab",
    "outputId": "8a047239-fb6b-4feb-fa9d-58742973f5ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              lyrics       topic\n",
      "0  hold time feel break feel untrue convince spea...     sadness\n",
      "1  believe drop rain fall grow believe darkest ni...  world/life\n",
      "2  sweetheart send letter goodbye secret feel bet...       music\n",
      "3  kiss lips want stroll charm mambo chacha merin...    romantic\n",
      "4  till darling till matter know till dream live ...    romantic\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('Music_Lyrics_cleaned.csv')\n",
    "\n",
    "\n",
    "# Fil\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbb4175",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "efbb4175",
    "outputId": "9688a6e0-b6ae-4b41-f8e8-dfa6cdbfd94e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\somil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>topic</th>\n",
       "      <th>lyrics_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hold time feel break feel untrue convince spea...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[hold, time, feel, break, feel, untrue, convin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>believe drop rain fall grow believe darkest ni...</td>\n",
       "      <td>world/life</td>\n",
       "      <td>[believe, drop, rain, fall, grow, believe, dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sweetheart send letter goodbye secret feel bet...</td>\n",
       "      <td>music</td>\n",
       "      <td>[sweetheart, send, letter, goodbye, secret, fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kiss lips want stroll charm mambo chacha merin...</td>\n",
       "      <td>romantic</td>\n",
       "      <td>[kiss, lip, want, stroll, charm, mambo, chacha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>till darling till matter know till dream live ...</td>\n",
       "      <td>romantic</td>\n",
       "      <td>[till, darling, till, matter, know, till, drea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics       topic  \\\n",
       "0  hold time feel break feel untrue convince spea...     sadness   \n",
       "1  believe drop rain fall grow believe darkest ni...  world/life   \n",
       "2  sweetheart send letter goodbye secret feel bet...       music   \n",
       "3  kiss lips want stroll charm mambo chacha merin...    romantic   \n",
       "4  till darling till matter know till dream live ...    romantic   \n",
       "\n",
       "                                       lyrics_tokens  \n",
       "0  [hold, time, feel, break, feel, untrue, convin...  \n",
       "1  [believe, drop, rain, fall, grow, believe, dar...  \n",
       "2  [sweetheart, send, letter, goodbye, secret, fe...  \n",
       "3  [kiss, lip, want, stroll, charm, mambo, chacha...  \n",
       "4  [till, darling, till, matter, know, till, drea...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running time 6 ms\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenization and lowercase\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and special characters\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stopwords_set and word not in string.punctuation]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply text preprocessing to the lyrics column\n",
    "df['lyrics_tokens'] = df['lyrics'].apply(preprocess_text)\n",
    "df.to_csv('en_token.csv',index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d1e9925",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d1e9925",
    "outputId": "ad3cd282-a620-42aa-89f1-34b354b8726a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " tokenized_lyrics = df['lyrics_tokens'].tolist()\n",
    " print(tokenized_lyrics)\n",
    "#  # Train Word2Vec model\n",
    " model = Word2Vec(sentences=tokenized_lyrics, vector_size=100, window=5, min_count=5, sg=0)\n",
    "\n",
    "# Save the trained Word2Vec model\n",
    " model.save(\"custom_lyrics_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22606598",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22606598",
    "outputId": "66b93828-306b-407d-e2b6-0b81f098b0ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humdinger', 0.6192483901977539),\n",
       " ('sweetest', 0.577287495136261),\n",
       " ('absolutely', 0.5709764957427979),\n",
       " ('forevermore', 0.5127493739128113),\n",
       " ('boopboopadoop', 0.5123421549797058),\n",
       " ('naturally', 0.5030989646911621),\n",
       " ('mambo', 0.5023256540298462),\n",
       " ('deeply', 0.5013391971588135),\n",
       " ('pigtail', 0.494067907333374),\n",
       " ('truly', 0.48832952976226807)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c2e0cc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "1c2e0cc3",
    "outputId": "d1d907ea-b195-4b28-d540-7805e6d4ddbb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Download the model file to your local machine\u001b[39;00m\n\u001b[0;32m      4\u001b[0m files\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_lyrics_word2vec.model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the model file to your local machine\n",
    "files.download('custom_lyrics_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a28c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c2a28c8",
    "outputId": "b7f1bf17-3f9a-420d-c729-6af8c4e628c5"
   },
   "outputs": [],
   "source": [
    "# Load your sentiment dataset\n",
    "data = pd.read_csv('Music_Lyrics_cleaned.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "data['lyrics'] = data['lyrics'].str.lower()\n",
    "data['lyrics'] = data['lyrics'].str.replace('[^a-zA-Z\\s]', '')\n",
    "w2v_model=model\n",
    "data.dropna(subset=['lyrics'], inplace=True)  # Remove rows with missing content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab189279",
   "metadata": {
    "id": "ab189279"
   },
   "outputs": [],
   "source": [
    "# Encode sentiment labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "data['topic_encoded'] = label_encoder.fit_transform(data['topic'])\n",
    "\n",
    "# Convert text data to document vectors using custom word embeddings\n",
    "sentence_vectors = []\n",
    "for sentence in data['lyrics']:\n",
    "    words = sentence.split()\n",
    "    vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if vectors:\n",
    "        sentence_vector = sum(vectors) / len(vectors)\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "    else:\n",
    "        # Handle cases where there are no valid words in the sentence\n",
    "        sentence_vectors.append([0] * w2v_model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52910db5",
   "metadata": {
    "id": "52910db5"
   },
   "outputs": [],
   "source": [
    "X = sentence_vectors\n",
    "y = data['topic_encoded']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0115a93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0115a93",
    "outputId": "adf463fa-fcbe-4266-e247-3da6d0a6ba5e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train a machine learning model (Logistic Regression in this example)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d626ee7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d626ee7",
    "outputId": "5970b045-c5b1-4d51-bab5-2fb3ab5fe12a"
   },
   "outputs": [],
   "source": [
    "# Provide sentiment analysis for new input using custom word embeddings\n",
    "new_lyrics = [\"Love me like you do\"]\n",
    "#new_lyrics=[\"remix killer alive remix thriller trap bitch spell damn cause hood nigga everybody list club fuck average watch fuck wrist average hoe fuck bitch window motherfuckin wall money jeans motherfuckin mall choppas bulletproof hummer competition summer wanna kill judge tryna lock homie feel thug thirsty fuck fuckin cause teach fuck stack money go hollywood funny know want pipe like crack bummy stop playin tryna like crash dummy start shit shit bitch bitch see snow bitch cancel like nino bitch kill club lyric commercial\"]\n",
    "\n",
    "#new_lyrics=[\"hook country boys country walk country talk bring round know jumpin country boys country walk country talk bring round know jumpin stille uhhh nigga game game hanes shirt roll chain chain doorag heavy blue south drive fast fast niggaz roll billies dutches dutches want brand cartel lemme key cutlass cutlass represent macktown macktown stay smokin smackdown little half pound half pound know stille drillin black folks livin court week givin fuck grow standin grow women grow women stay high play till home wittem whattchu thinkin whattchu drinkin thinkin trickin trippin thinkin come hook skinny deville nigga hook like waitress ihop nothin grit steak waitin dollar pancake frontback lyric commercial\"]\n",
    "#new_lyrics=[\"tear heart seat stay awhile tear heart game steal glimpse eye stare awhile steal glimpse eye game half awhile half game word mouth swallow speak awhile word mouth game offer lose say mean right look size right tear heart seat stay awhile tear heart game steal glimpse eye stare awhile steal glimpse eye game offer lose say mean right look size\"]\n",
    "\n",
    "new_lyrics_words = new_lyrics[0].split()\n",
    "new_lyrics_vectors = [w2v_model.wv[word] for word in new_lyrics_words if word in w2v_model.wv]\n",
    "if new_lyrics_vectors:\n",
    "    new_lyrics_vector = sum(new_lyrics_vectors) / len(new_lyrics_vectors)\n",
    "else:\n",
    "    new_lyrics_vector = [0] * w2v_model.vector_size\n",
    "\n",
    "predicted_sentiment_encoded = model.predict([new_lyrics_vector])\n",
    "predicted_sentiment = label_encoder.inverse_transform(predicted_sentiment_encoded)\n",
    "print(predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e0386",
   "metadata": {
    "id": "212e0386"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f86f1f",
   "metadata": {
    "id": "e8f86f1f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93657072",
   "metadata": {
    "id": "93657072"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93df59",
   "metadata": {
    "id": "7a93df59"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e894592",
   "metadata": {
    "id": "7e894592"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef215f",
   "metadata": {
    "id": "4eef215f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d81f9b",
   "metadata": {
    "id": "47d81f9b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543fe58",
   "metadata": {
    "id": "9543fe58"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab0408",
   "metadata": {
    "id": "d0ab0408"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7722f50",
   "metadata": {
    "id": "f7722f50"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588adf48",
   "metadata": {
    "id": "588adf48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666444e",
   "metadata": {
    "id": "d666444e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cfd80",
   "metadata": {
    "id": "af9cfd80"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
